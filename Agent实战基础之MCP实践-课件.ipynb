{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99ee6a21-fedd-4e7f-9284-57ced894e9c1",
   "metadata": {},
   "source": [
    "# Agent实战基础之MCP实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4b1aca-5049-4d43-bbbc-f2317ae68fc2",
   "metadata": {},
   "source": [
    "## 1. MCP基础知识"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffba819-bbb2-4fd4-9bb1-7d6b52d61882",
   "metadata": {},
   "source": [
    "### 1.1 uv基础介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb58940-b9af-4d6d-ae07-0fbd720b03f4",
   "metadata": {},
   "source": [
    "uv入门基础知识\n",
    "MCP开发要求借助uv进行虚拟环境创建和依赖管理。 uv 是一个Python 依赖管理工具，类似于\n",
    "pip 和 conda ，但它更快、更高效，并且可以更好地管理 Python 虚拟环境和依赖项。它的核心目标是\n",
    "替代 pip 、 venv 和 pip-tools ，提供更好的性能和更低的管理开销。\n",
    "\n",
    "uv 的特点：\n",
    "1. 速度更快：相比 pip ， uv 采用 Rust 编写，性能更优。\n",
    "2. 支持 PEP 582：无需 virtualenv ，可以直接使用 __pypackages__ 进行管理。\n",
    "3. 兼容 pip ：支持 requirements.txt 和 pyproject.toml 依赖管理。\n",
    "4. 替代 venv ：提供 uv venv 进行虚拟环境管理，比 venv 更轻量。\n",
    "5. 跨平台：支持 Windows、macOS 和 Linux。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a6ec18-2ad1-4bcb-bab4-fd26c4b148d5",
   "metadata": {},
   "source": [
    "**为什么MCP推荐使用uv进行环境管理？**\n",
    "\n",
    "MCP 依赖的 Python 环境可能包含多个模块， uv 通过 pyproject.toml 提供更高效的管理方式，并且可以避免 pip 的一些依赖冲突问题。此外， uv 的包管理速度远超 pip ，这对于 MCP这样频繁管理依赖的项目来说是一个很大的优势。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b9b313-5bb2-41e7-a697-ebd2fbd3ad19",
   "metadata": {},
   "source": [
    "## 1.2 uv常见命令"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d601094f-1646-4b8f-a817-63999ca93128",
   "metadata": {},
   "source": [
    "1. 安装uv\n",
    "pip install uv\n",
    "\n",
    "2. 安装 Python 依赖\n",
    "uv pip install requests\n",
    "\n",
    "3. 创建虚拟环境\n",
    "uv venv myenv #等效于 python -m venv myenv \n",
    "\n",
    "4. 激活虚拟环境\n",
    "source myenv/bin/activate\n",
    "\n",
    "5. 安装 requirements.txt\n",
    "uv pip install -r requirements.txt\n",
    "\n",
    "6. 运行 Python 项目\n",
    "uv run python myscript.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf318c98-806f-4436-b009-3005f3ccb926",
   "metadata": {},
   "source": [
    "## 2. MCP client设计"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fbc3d9-f9b7-42ad-8b46-06187b6c01d6",
   "metadata": {},
   "source": [
    "### 2.1 创建简单的MCP Client项目"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09497f2c-9951-4be5-be8c-5f4102d57dca",
   "metadata": {},
   "source": [
    "# 创建项目目录\n",
    "uv init mcp-lesson\n",
    "cd mcp-lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8120478-a7ee-43ba-adb2-32ddeef65c8d",
   "metadata": {},
   "source": [
    "<center><img src=\"./pic/1.png\" style=\"zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9a486a-66d1-4c24-a014-95453d2b7775",
   "metadata": {},
   "source": [
    "<center><img src=\"./pic/2.png\" style=\"zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf48d9e-5af3-4551-b6a8-bcc208e190c7",
   "metadata": {},
   "source": [
    "### 2.2 创建虚拟环境并激活"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7b12f72-a998-45bc-9c55-920b169770cb",
   "metadata": {},
   "source": [
    "# 创建虚拟环境\n",
    "uv venv\n",
    "# 激活虚拟环境\n",
    "source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a902df-6296-4a40-8b52-b92b724a31c0",
   "metadata": {},
   "source": [
    "<center><img src=\"./pic/3.png\" style=\"zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58246b7b-2a61-4720-8847-fa95d5166c4f",
   "metadata": {},
   "source": [
    "# 安装 MCP SDK\n",
    "uv add mcp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f9327-1cc0-4aff-af8c-67a4279d26d9",
   "metadata": {},
   "source": [
    "<center><img src=\"./pic/4.png\" style=\"zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5373330-4885-4283-9429-697ca2223410",
   "metadata": {},
   "source": [
    "### 2.3 编写简易Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce82017b-dae4-489f-ba5f-e78d32acaf78",
   "metadata": {},
   "source": [
    "<center><img src=\"./pic/5.png\" style=\"zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a96bc984-6a07-4f26-9cbe-1a8a9cc5d8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP Client已初始化，但未连接到服务器\n",
      "\n",
      "MCP Client已启动！输入 'quit' 退出\n",
      "\n",
      " [模拟回复] 你说的是：Enter\n",
      "\n",
      " [模拟回复] 你说的是：hello mcp\n",
      "\n",
      " [模拟回复] 你说的是：hi,what is mcp\n"
     ]
    }
   ],
   "source": [
    "import asyncio #支持异步操作\n",
    "from mcp import ClientSession # MCP Client会话管理\n",
    "from contextlib import AsyncExitStack  # 资源管理（确保客户端关闭时释放资源）\n",
    "\n",
    "class MCPClient:\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化 MCP Client\"\"\"\n",
    "        self.session = None\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        \n",
    "    async def connect_to_mock_server(self):\n",
    "        \"\"\"模拟 MCP 服务器的连接（暂不连接真实服务器）\"\"\"\n",
    "        print(\"MCP Client已初始化，但未连接到服务器\")\n",
    "        \n",
    "    async def chat_loop(self):\n",
    "        \"\"\"运行交互式聊天循环\"\"\"\n",
    "        print(\"\\nMCP Client已启动！输入 'quit' 退出\")\n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\nPrompt: \").strip()\n",
    "                if query.lower() == 'quit':\n",
    "                    break\n",
    "                print(f\"\\n [模拟回复] 你说的是：{query}\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n发生错误: {str(e)}\")\n",
    "                \n",
    "    async def cleanup(self):\n",
    "        \"\"\"清理资源\"\"\"\n",
    "        await self.exit_stack.aclose()\n",
    "                \n",
    "async def main():\n",
    "    client = MCPClient()\n",
    "    try:\n",
    "        await client.connect_to_mock_server()\n",
    "        await client.chat_loop()\n",
    "    finally:\n",
    "        await client.cleanup()\n",
    "            \n",
    "await main()\n",
    "# if __name__ == \"__main__\":\n",
    "    # asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ed0be-f0f2-4841-8de8-90a8bc025a76",
   "metadata": {},
   "source": [
    "### 2.4 运行客户端"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622fc14f-e34d-4133-982f-6598f69d4595",
   "metadata": {},
   "source": [
    "uv run client_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b87381-ee48-4731-a5c9-8422f2ec196c",
   "metadata": {},
   "source": [
    "<center><img src=\"./pic/6.png\" style=\"zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3582405-3ea3-41a1-9b8d-5d306faf6ab8",
   "metadata": {},
   "source": [
    "## 3. MCP Client接入大模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e3f919-0bf7-4333-999b-c7bbd4b6d5c7",
   "metadata": {},
   "source": [
    "### 3.1 添加依赖"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4eeb707-6cb1-442f-aad6-336b01760042",
   "metadata": {},
   "source": [
    "## 需要到一些大模型客户端的依赖  deepseek qwen ollama\n",
    "uv add mcp openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0921e0bd-38c3-4915-bcb6-fd69625e82a6",
   "metadata": {},
   "source": [
    "<center><img src=\"./pic/7.png\" style=\"zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6238c917-9816-41df-aa45-40f2e7ca7551",
   "metadata": {},
   "source": [
    "### 3.2 创建隐藏文件.env"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35a99d7f-aae9-409c-9aed-566a8bec924f",
   "metadata": {},
   "source": [
    "touch .env\n",
    "vi .env"
   ]
  },
  {
   "cell_type": "raw",
   "id": "550d2b1c-f1aa-46f8-a631-2a0c5015a166",
   "metadata": {},
   "source": [
    "BASE_URL=\"代理地址\"\n",
    "MODEL=gpt-4o\n",
    "OPENAI_API_KEY=\"xxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6461d7-8025-49e5-b881-72158fe03997",
   "metadata": {},
   "source": [
    "### 3.3 Client代码设计如下 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3c73d8c-36e4-460e-bdc5-96d2085763a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MCP Client已启动！输入 'quit' 退出\n",
      "\n",
      " 智能助手: 我是一个智能助手，旨在帮助你解答问题，提供信息和支持。无论你有任何疑问或需要的帮助，请随时告诉我！\n",
      "\n",
      " 智能助手: MCP可以指代多种不同的概念，具体取决于上下文。以下是一些常见的含义：\n",
      "\n",
      "1. **Microsoft Certified Professional（微软认证专家）**：这是一个由微软提供的认证，旨在证明IT专业人员在使用微软技术方面的知识和技能。获得MCP认证的人士通常会在软件开发、系统管理等领域拥有更好的职业发展机会。\n",
      "\n",
      "2. **Multi-Chip Package（多芯片封装）**：在电子工程中，MCP是一种将多个集成电路芯片封装在一起的技术，以提高提供的功能和降低电路板的空间占用。\n",
      "\n",
      "3. **Master Control Program（主控程序）**：在计算机科学和操作系统中，这个术语可能指代能够管理和调度其他程序运行的主程序。\n",
      "\n",
      "4. **Model Control Protocol（模型控制协议）**：在某些网络和通信领域，这可能涉及网络设备间通信的协议。\n",
      "\n",
      "5. **其他领域的专有名词**：MCP也可能是某些特定行业或公司的缩写，涉及特定领域的技术、产品或服务。\n",
      "\n",
      "如果你有特定的上下文或领域，请告诉我，我可以提供更具体的信息。\n",
      "\n",
      " 智能助手: MCP可以指代多个不同的概念，具体含义通常取决于上下文。以下是一些常见的含义：\n",
      "\n",
      "1. **微控制器平台（Microcontroller Platform）**：在嵌入式系统领域，MCP可能指的是一些用于开发和拥有微控制器的开发平台。\n",
      "\n",
      "2. **多重认证程序（Multi-factor Certification Program）**：在网络安全领域，MCP可以是多重认证相关的程序，用于增强数据保护和访问控制。\n",
      "\n",
      "3. **MCP（Microsoft Certified Professional）**：这是微软公司提供的一个认证，旨在验证IT专业人员在微软技术方面的专业知识和能力。\n",
      "\n",
      "4. **MCP（Master Control Program）**：在计算机科学和编程领域，MCP可以指一种用于控制系统和设备的程序或系统。\n",
      "\n",
      "如果你有特定的上下文或更具体的领域，欢迎提供详细信息，我可以为你提供更准确的介绍！\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from contextlib import AsyncExitStack\n",
    "# 加载隐藏文件，确保 API Key 受到保护\n",
    "load_dotenv()\n",
    "class MCPClient:\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化 MCP Client\"\"\"\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\") # 读取 Key\n",
    "        # self.base_url = os.getenv(\"BASE_URL\") # 读取 BASE URL\n",
    "        self.model = os.getenv(\"MODEL\") # 读取 model\n",
    "\n",
    "        if not self.openai_api_key:\n",
    "            raise ValueError(\"未找到 OpenAI API Key，请在 .env 文件中设置OPENAI_API_KEY\")\n",
    "        # self.client = OpenAI(api_key=self.openai_api_key, base_url=self.base_url)\n",
    "        self.client = OpenAI(api_key=self.openai_api_key)\n",
    "        \n",
    "    async def process_query(self, query: str) -> str:\n",
    "        \"\"\"调用 OpenAI API 处理用户查询\"\"\"\n",
    "        messages = [{\"role\": \"system\", \"content\": \"你是一个智能助手，帮助用户回答问题。\"},\n",
    "                {\"role\": \"user\", \"content\": query}]\n",
    "        try:\n",
    "            # 调用大模型API\n",
    "            response = await asyncio.get_event_loop().run_in_executor(\n",
    "                \n",
    "                None,lambda: self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages) )\n",
    "            \n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"调用 OpenAI API 时出错: {str(e)}\"\n",
    "            \n",
    "    async def chat_loop(self):\n",
    "        \"\"\"运行交互式聊天循环\"\"\"\n",
    "        print(\"\\n MCP Client已启动！输入 'quit' 退出\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\n用户: \").strip()\n",
    "                if query.lower() == 'quit':\n",
    "                    break\n",
    "                response = await self.process_query(query) \n",
    "                print(f\"\\n 智能助手: {response}\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n 发生错误: {str(e)}\")\n",
    "                \n",
    "    async def cleanup(self):\n",
    "        \"\"\"清理资源\"\"\"\n",
    "        await self.exit_stack.aclose()\n",
    "        \n",
    "async def main():\n",
    "    client = MCPClient()\n",
    "    try:\n",
    "        await client.chat_loop()\n",
    "    finally:\n",
    "        await client.cleanup()\n",
    "if __name__ == \"__main__\":\n",
    "    task = asyncio.create_task(main())\n",
    "    await task\n",
    "    # asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4605efc5-55c9-47aa-bc3b-2e3adb42819f",
   "metadata": {},
   "source": [
    "### 3.4 运行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cef5361-015b-4966-8767-eb12eaae3d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv run client_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64874995-aa35-46d5-b729-08e967026146",
   "metadata": {},
   "source": [
    "<center><img src=\"./pic/8.png\" style=\"zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619aa214-d127-4b78-ab63-4381b321e801",
   "metadata": {},
   "source": [
    "## 4. MCP Server端设计"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d639ce-5f9c-4c67-a906-8c997d447361",
   "metadata": {},
   "source": [
    "### 4.1 MCP Server概念介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca090a5b-0e28-4901-8d54-758cdd134f82",
   "metadata": {},
   "source": [
    "根据MCP协议定义，Server可以提供三种类型的标准能力，Resources、Tools、Prompts，每个\n",
    "Server可同时提供者三种类型能力或其中一种。\n",
    "\n",
    "**Resources**：资源，类似于文件数据读取，可以是文件资源或是API响应返回的内容。\n",
    "\n",
    "**Tools**：工具，第三方服务、功能函数，通过此可控制LLM可调用哪些函数。\n",
    "\n",
    "**Prompts**：提示词，为用户预先定义好的完成特定任务的模板。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163be0c5-f374-4dc7-a8eb-ea1fc986df51",
   "metadata": {},
   "source": [
    "### 4.2 MCP Server的通讯机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eabe95-7f85-47ca-b008-319a57d94d57",
   "metadata": {},
   "source": [
    "   Model Context Protocol（MCP）由Anthropic开源，用于将大型语言模型直接连接数据源。它支持**标准输入输出（stdio）** 和 **基于HTTP的服务器推送事件（SSE）** 两种传输方式。Stdio模式适用于本地通信，通过启动服务器作为子进程实现高效低延迟的数据交换，适合快速响应的本地应用。而基于HTTP和SSE的方式则适用于分布式或远程场景，实现客户端与服务器间实时数据推送。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f6d40d-0c8e-4ba3-973b-c7f7b9bdb920",
   "metadata": {},
   "source": [
    "    1. 本地通讯：使用了stdio传输数据，具体流程Client启动Server程序作为子进程，其消息通讯是通过stdin/stdout进行的，消息格式为JSON-RPC 2.0。\n",
    "    \n",
    "    2. 远程通讯：Client与Server可以部署在任何地方，Client使用SSE与Server进行通讯，消息的格式为JSON-RPC 2.0，Server定义了/see与/messages接口用于推送与接收数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45122f0b-3dae-4e65-8457-df947de08efa",
   "metadata": {},
   "source": [
    "## 4.3 我们基于stdio实现一个【天气智能助手】"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb27af-2d83-4b03-85b5-5a7149b9a8dd",
   "metadata": {},
   "source": [
    "#### 4.3.1 天气智能助手流程设计 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326617c5-7ce6-453f-9468-5bd6d51b9271",
   "metadata": {},
   "source": [
    "<center><img src=\"./pic/9.png\" style=\"zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced4710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ 重要说明：解决Jupyter中的事件循环冲突问题\n",
    "print(\"=== MCP服务器运行说明 ===\")\n",
    "print()\n",
    "print(\"❌ 问题：在Jupyter notebook中直接运行MCP服务器会出现：\")\n",
    "print(\"   'RuntimeError: Already running asyncio in this thread'\")\n",
    "print()\n",
    "print(\"✅ 解决方案：\")\n",
    "print(\"1. MCP服务器必须作为独立的Python进程运行\")\n",
    "print(\"2. 服务器代码已保存在 main.py 文件中\")\n",
    "print(\"3. 请在终端中执行以下命令：\")\n",
    "print()\n",
    "print(\"   # 启动MCP服务器（在一个终端窗口中）\")\n",
    "print(\"   uv run main.py\")\n",
    "print()\n",
    "print(\"   # 连接客户端（在另一个终端窗口中）\")\n",
    "print(\"   uv run client.py main.py\")\n",
    "print()\n",
    "print(\"📋 文件说明：\")\n",
    "print(\"- main.py: MCP天气服务器（包含天气查询工具）\")\n",
    "print(\"- client.py: MCP客户端（连接服务器并与大模型交互）\")\n",
    "print(\"- .env: 环境变量文件（存储API密钥）\")\n",
    "print()\n",
    "print(\"🔧 调试工具：\")\n",
    "print(\"   # 使用MCP Inspector进行测试\")\n",
    "print(\"   npx @modelcontextprotocol/inspector uv run main.py\")\n",
    "print()\n",
    "print(\"💡 原理：MCP使用stdio通信，服务器作为子进程启动，\")\n",
    "print(\"   避免与Jupyter的事件循环冲突。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e1f962-59c9-4902-aa56-7112a0acbb96",
   "metadata": {},
   "source": [
    "#### 4.3.2 天气智能助手MCP服务端代码设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b222110-7438-43d4-9dcb-268204af6ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/workspace/mcp-quickstart/.venv/lib/python3.10/site-packages/executing/executing.py:506: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  return compile(\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Already running asyncio in this thread",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m format_weather(data)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mmcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstdio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/mcp-quickstart/.venv/lib/python3.10/site-packages/mcp/server/fastmcp/server.py:217\u001b[0m, in \u001b[0;36mFastMCP.run\u001b[0;34m(self, transport, mount_path)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mmatch\u001b[39;00m transport:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstdio\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[43manyio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_stdio_async\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msse\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    219\u001b[0m         anyio\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_sse_async(mount_path))\n",
      "File \u001b[0;32m~/workspace/mcp-quickstart/.venv/lib/python3.10/site-packages/anyio/_core/_eventloop.py:59\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(func, backend, backend_options, *args)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlready running \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masynclib_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in this thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     async_backend \u001b[38;5;241m=\u001b[39m get_async_backend(backend)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Already running asyncio in this thread"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import httpx\n",
    "from typing import Any\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "# 初始化 MCP 服务器\n",
    "mcp = FastMCP(\"WeatherServer\")\n",
    "# OpenWeather API 配置\n",
    "OPENWEATHER_API_BASE = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "API_KEY = \"5c939a7cc59eb8696f4cd77bf75c5a9a\" # 请替换为你自己的 OpenWeather API Key\n",
    "USER_AGENT = \"weather-app/1.0\"\n",
    "\n",
    "async def fetch_weather(city: str) -> dict[str, Any] | None:\n",
    "    \"\"\"\n",
    "    从 OpenWeather API 获取天气信息。\n",
    "    :param city: 城市名称（需使用英文，如 Beijing）\n",
    "    :return: 天气数据字典；若出错返回包含 error 信息的字典\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"q\": city,\n",
    "        \"appid\": API_KEY,\n",
    "        \"units\": \"metric\",\n",
    "        \"lang\": \"zh_cn\"\n",
    "     }\n",
    "    headers = {\"User-Agent\": USER_AGENT}\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        try:\n",
    "            response = await client.get(OPENWEATHER_API_BASE, params=params,\n",
    "            headers=headers, timeout=30.0)\n",
    "            response.raise_for_status()\n",
    "            return response.json() # 返回字典类型\n",
    "        except httpx.HTTPStatusError as e:\n",
    "            return {\"error\": f\"HTTP 错误: {e.response.status_code}\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"请求失败: {str(e)}\"}\n",
    "            \n",
    "def format_weather(data: dict[str, Any] | str) -> str:\n",
    "    \"\"\"\n",
    "    将天气数据格式化为易读文本。\n",
    "    :param data: 天气数据（可以是字典或 JSON 字符串）\n",
    "    :return: 格式化后的天气信息字符串\n",
    "    \"\"\"\n",
    "    # 如果传入的是字符串，则先转换为字典\n",
    "    if isinstance(data, str):\n",
    "        try:\n",
    "            data = json.loads(data)\n",
    "        except Exception as e:\n",
    "            return f\"无法解析天气数据: {e}\"\n",
    "            # 如果数据中包含错误信息，直接返回错误提示\n",
    "    if \"error\" in data:\n",
    "        return f\"{data['error']}\"\n",
    "    # 提取数据时做容错处理\n",
    "    city = data.get(\"name\", \"未知\")\n",
    "    country = data.get(\"sys\", {}).get(\"country\", \"未知\")\n",
    "    temp = data.get(\"main\", {}).get(\"temp\", \"N/A\")\n",
    "    humidity = data.get(\"main\", {}).get(\"humidity\", \"N/A\")\n",
    "    wind_speed = data.get(\"wind\", {}).get(\"speed\", \"N/A\")\n",
    "    # weather 可能为空列表，因此用 [0] 前先提供默认字典\n",
    "    weather_list = data.get(\"weather\", [{}])\n",
    "    description = weather_list[0].get(\"description\", \"未知\")\n",
    "    return (\n",
    "        f\"城市{city}, {country}\\n\"\n",
    "        f\"温度: {temp}°C\\n\"\n",
    "        f\"湿度: {humidity}%\\n\"\n",
    "        f\"风速: {wind_speed} m/s\\n\"\n",
    "        f\"天气: {description}\\n\")\n",
    "\n",
    "@mcp.tool()\n",
    "async def query_weather(city: str) -> str:\n",
    "    \"\"\"\n",
    "    输入指定城市的英文名称，返回今日天气查询结果。\n",
    "    :param city: 城市名称（需使用英文）\n",
    "    :return: 格式化后的天气信息\n",
    "    \"\"\"\n",
    "    data = await fetch_weather(city)\n",
    "    return format_weather(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(transport='stdio')\n",
    "    # 启动 MCP 服务器\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a3da4b-6692-4151-9fd5-cf70d3e83948",
   "metadata": {},
   "source": [
    "#### 4.3.3 天气智能助手MCP客户端代码设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebfc453-2ab9-4e44-a9c7-b07ef38c4b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import json\n",
    "from typing import Optional\n",
    "from contextlib import AsyncExitStack\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class MCPClient:\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化 MCP 客户端\"\"\"\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\") # 读取Key\n",
    "        self.base_url = os.getenv(\"BASE_URL\") # 读取 URL\n",
    "        self.model = os.getenv(\"MODEL\") # 读取 model\n",
    "        if not self.openai_api_key:\n",
    "            raise ValueError(\"未找到 OpenAI API Key\")\n",
    "        self.client = OpenAI(api_key=self.openai_api_key, base_url=self.base_url)\n",
    "        \n",
    "        self.session: Optional[ClientSession] = None\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "\n",
    "    async def connect_to_server(self, server_script_path: str):\n",
    "        \"\"\"连接到 MCP 服务器并列出可用工具\"\"\"\n",
    "        is_python = server_script_path.endswith('.py')\n",
    "        is_js = server_script_path.endswith('.js')\n",
    "        ## 判断服务器脚本是 Python 还是 Node.js，选择对应的运行命令\n",
    "        if not (is_python or is_js):\n",
    "            raise ValueError(\"服务器脚本必须是 .py 或 .js 文件\")\n",
    "        command = \"python\" if is_python else \"node\"\n",
    "        ## 告诉 MCP 客户端如何启动服务器\n",
    "        server_params = StdioServerParameters(\n",
    "            command=command,\n",
    "            args=[server_script_path],\n",
    "            env=None\n",
    "        )\n",
    "        # 启动 MCP 服务器并建立通信\n",
    "        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))\n",
    "        self.stdio, self.write = stdio_transport\n",
    "        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))\n",
    "        await self.session.initialize()\n",
    "        # 向 MCP 服务器请求所有已注册的工具（用 @mcp.tool() 标记）\n",
    "        response = await self.session.list_tools()\n",
    "        tools = response.tools\n",
    "        print(\"\\n已连接到服务器，支持以下工具:\", [tool.name for tool in tools])\n",
    " \n",
    "    ## 这个本质就是Function call\n",
    "    async def process_query(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        使用大模型处理查询并调用可用的 MCP 工具 (Function Calling)\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": query}]\n",
    "        response = await self.session.list_tools()\n",
    "        available_tools = [{\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "            \"name\": tool.name,\n",
    "            \"description\": tool.description,\n",
    "            \"input_schema\": tool.inputSchema\n",
    "            }\n",
    "        } for tool in response.tools]\n",
    "        # print(available_tools)\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            tools=available_tools)\n",
    "        # 处理返回的内容\n",
    "        content = response.choices[0]\n",
    "        if content.finish_reason == \"tool_calls\":\n",
    "            # 如何是需要使用工具，就解析工具\n",
    "            tool_call = content.message.tool_calls[0]\n",
    "            tool_name = tool_call.function.name\n",
    "            tool_args = json.loads(tool_call.function.arguments)\n",
    "             # 执行工具\n",
    "            result = await self.session.call_tool(tool_name, tool_args)\n",
    "            print(f\"\\n\\n[Calling tool {tool_name} with args {tool_args}]\\n\\n\")\n",
    "            # 将模型返回的调用哪个工具数据和工具执行完成后的数据都存入messages中\n",
    "            messages.append(content.message.model_dump())\n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": result.content[0].text,\n",
    "                \"tool_call_id\": tool_call.id,})\n",
    "            \n",
    "            # 将上面的结果再返回给大模型用于生产最终的结果\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        return content.message.content\n",
    "        \n",
    "    async def chat_loop(self):\n",
    "        \"\"\"运行交互式聊天循环\"\"\"\n",
    "        print(\"\\n 智能助手客户端已启动！输入 'quit' 退出\")\n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\n用户: \").strip()\n",
    "                if query.lower() == 'quit':\n",
    "                    break\n",
    "                response = await self.process_query(query) # 发送用户输入到 OpenAIAPI\n",
    "                print(f\"\\n天气预报智能助手: {response}\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n发生错误: {str(e)}\")\n",
    "                \n",
    "    async def cleanup(self):\n",
    "        \"\"\"清理资源\"\"\"\n",
    "        await self.exit_stack.aclose()\n",
    "            \n",
    "async def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python client.py <path_to_server_script>\")\n",
    "        sys.exit(1)\n",
    "    client = MCPClient()\n",
    "    try:\n",
    "        await client.connect_to_server(sys.argv[1])\n",
    "        await client.chat_loop()\n",
    "    finally:\n",
    "        await client.cleanup()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb24d3c-22cf-42d3-b5e9-e6319d6f0f7e",
   "metadata": {},
   "source": [
    "#### 4.3.4 流程测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef59f2-932c-4c3b-8543-0295c83368a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "uv run client_3.py server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89a35a5-01d5-4cf4-be61-5ed4e8ecae09",
   "metadata": {},
   "source": [
    "<center><img src=\"./pic/10.png\" style=\"zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ba3e33-3063-4c0d-81cd-ada27d6ad233",
   "metadata": {},
   "source": [
    "## 5. MCP Inspector功能介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08a2411-75dd-474d-9a5f-e8c08b71a329",
   "metadata": {},
   "source": [
    "   在实际开发MCP服务器的过程中，Anthropic提供了一个非常便捷的debug工具：Inspector。借助\n",
    "Inspector，我们能够非常快捷的调用各类server，并测试其功能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5653f3c4-f7b5-44e0-bc3f-7208bdd18be7",
   "metadata": {},
   "source": [
    "### 5.1 安装node.js "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc9010-424a-4974-8ce5-93ceaded54a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -fsSL https://deb.nodesource.com/setup_20.x | sudo bash -\n",
    "sudo apt install -y nodejs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d1a74f-5ef6-499c-a49a-43411b93aa9b",
   "metadata": {},
   "source": [
    "<center><img src=\"./pic/11.png\" style=\"zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2753888a-75c0-4461-93d0-195107cbf909",
   "metadata": {},
   "source": [
    "<center><img src=\"./pic/12.png\" style=\"zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02f31e4-ae17-4f94-99cd-c7da2d820f79",
   "metadata": {},
   "source": [
    "### 5.2 启动Inspector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13b2327-be6c-4abb-a11a-7f3ab8d82429",
   "metadata": {},
   "outputs": [],
   "source": [
    "npx -y @modelcontextprotocol/inspector uv run server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1abe80-e0f4-433e-9187-868746bde329",
   "metadata": {},
   "source": [
    "<center><img src=\"./pic/13.png\" style=\"zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d58943-6adf-4a28-b44d-c9f093b139e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "autodl是云平台，我们直接访问平台，需要做端口映射"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e783b62-d9b1-4225-a0ea-72c55066829d",
   "metadata": {},
   "source": [
    "<center><img src=\"./pic/14.png\" style=\"zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4ac0ed-57e2-42e9-b8bd-c9668700da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "工具测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8414a550-6fb2-4892-a7b5-07c32b4d8451",
   "metadata": {},
   "source": [
    "<center><img src=\"./pic/15.png\" style=\"zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10118090-234a-4808-9edd-d1b472eed120",
   "metadata": {},
   "source": [
    "<center><img src=\"./pic/15.png\" style=\"zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe1d9a-ea11-474c-861b-0497a35e5d20",
   "metadata": {},
   "source": [
    "<center><img src=\"./pic/16.png\" style=\"zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfac327-6fb9-44a6-8d0f-ea4c9da88b0e",
   "metadata": {},
   "source": [
    "<center><img src=\"./pic/17.png\" style=\"zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d8258a3-f900-4c28-839e-b16d9f1d8611",
   "metadata": {},
   "source": [
    "想要学习更多的大模型知识，加西瓜老师微信"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fb09b5-9859-41b1-9c52-f68abf6ecbf1",
   "metadata": {},
   "source": [
    "<center><img src=\"./pic/18.png\" style=\"zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f6bb34-051b-4e1c-b515-746bfc74e603",
   "metadata": {},
   "source": [
    "## 扩展"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878984e6-93a0-4c95-9806-cb0806219e1e",
   "metadata": {},
   "source": [
    "Anthropic MCP发布：https://www.anthropic.com/news/model-context-protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af08aa12-71f4-4039-bc81-c5acfc461271",
   "metadata": {},
   "source": [
    "官方Server: https://github.com/modelcontextprotocol/servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f0631-610c-4c21-8434-3b06138e3de9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
